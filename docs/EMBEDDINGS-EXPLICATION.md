# üßÆ Comment sont g√©n√©r√©s les embeddings ?

> Une explication simple de la g√©n√©ration des embeddings dans le RAG Server

## Les deux m√©thodes disponibles

Le projet propose **deux m√©thodes** pour g√©n√©rer les embeddings :

1. **TF-IDF Simple** (par d√©faut) - 100% C#, formule math√©matique
2. **Sentence Transformers** (optionnel) - Mod√®le neuronal via Python

---

## 1Ô∏è‚É£ TF-IDF Simple (par d√©faut, 100% C#)

**Fichier** : `src/Mo5.RagServer.Infrastructure/Services/SimpleTfIdfEmbeddingService.cs`

### C'est quoi ?

Une **formule math√©matique** pure, impl√©ment√©e en C#.

**Pas de NuGet externe**, juste des calculs math√©matiques ! üéØ

### Comment √ßa marche ?

#### √âtape 1 : Construction du vocabulaire

Au d√©marrage, le syst√®me lit tous les chunks et construit un **vocabulaire** (liste de tous les mots uniques).

```csharp
// Construire le vocabulaire
var allTerms = new HashSet<string>();
foreach (var text in texts)
{
    var terms = TokenizeAndFilter(text);
    foreach (var term in terms)
    {
        allTerms.Add(term);
    }
}
```

**Exemple** :

```
Document 1 : "Le registre A est un accumulateur"
Document 2 : "Le registre X est un index"

Vocabulaire : ["registre", "accumulateur", "index"]
```

Les **mots vides** (le, un, est, etc.) sont filtr√©s automatiquement.

#### √âtape 2 : Calcul des scores IDF

Pour chaque mot du vocabulaire, on calcule son **IDF** (Inverse Document Frequency).

**Formule** : `IDF = log(nombre total de documents / nombre de documents contenant le mot)`

```csharp
// Calculer les scores IDF
var totalDocuments = documentTerms.Count;
foreach (var term in _vocabulary.Keys)
{
    var documentsContainingTerm = documentTerms.Count(doc => doc.Contains(term));
    _idfScores[term] = Math.Log((double)totalDocuments / (1 + documentsContainingTerm));
}
```

**Exemple** :

```
Total de documents : 100

Mot "registre" : pr√©sent dans 80 documents
‚Üí IDF = log(100 / 80) = 0.22  (mot commun, score faible)

Mot "accumulateur" : pr√©sent dans 5 documents
‚Üí IDF = log(100 / 5) = 3.00  (mot rare, score √©lev√©)
```

**Principe** : Les mots rares sont plus importants pour le sens.

#### √âtape 3 : G√©n√©ration de l'embedding pour un texte

Pour chaque chunk, on calcule le **TF-IDF** de chaque mot.

**Formule** : `TF-IDF = (fr√©quence du mot dans le texte / nombre total de mots) √ó IDF`

```csharp
// Calculer TF-IDF pour chaque terme du vocabulaire
var totalTerms = terms.Count;
foreach (var kvp in _vocabulary)
{
    var term = kvp.Key;
    var index = kvp.Value;

    if (termFrequency.ContainsKey(term))
    {
        var tf = (double)termFrequency[term] / totalTerms;
        var idf = _idfScores.GetValueOrDefault(term, 0);
        vector[index] = (float)(tf * idf);
    }
}
```

**Exemple** :

```
Texte : "Le registre A est un accumulateur 8 bits"

Mots apr√®s filtrage : ["registre", "accumulateur", "bits"]

Calcul TF-IDF :
- "registre" : TF = 1/3 = 0.33, IDF = 0.22 ‚Üí TF-IDF = 0.07
- "accumulateur" : TF = 1/3 = 0.33, IDF = 3.00 ‚Üí TF-IDF = 1.00
- "bits" : TF = 1/3 = 0.33, IDF = 1.50 ‚Üí TF-IDF = 0.50

Vecteur : [0.07, 1.00, 0.50, 0, 0, 0, ..., 0]  (384 dimensions)
```

#### √âtape 4 : Normalisation

Le vecteur est **normalis√©** pour que sa longueur soit 1.

**Formule** : `vecteur normalis√© = vecteur / magnitude`

```csharp
// Normaliser le vecteur
var magnitude = Math.Sqrt(vector.Sum(x => x * x));
if (magnitude > 0)
{
    for (int i = 0; i < vector.Length; i++)
    {
        vector[i] = (float)(vector[i] / magnitude);
    }
}
```

**Pourquoi normaliser ?**

Pour que tous les vecteurs aient la m√™me "longueur", et qu'on puisse les comparer √©quitablement.

**R√©sultat** : Un vecteur de **384 nombres** (float) qui repr√©sente le texte.

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- 100% C#, pas de d√©pendance externe
- Tr√®s rapide
- Fonctionne hors ligne
- Pas de configuration complexe

**‚ùå Inconv√©nients** :
- Bas√© sur les mots-cl√©s (pas de compr√©hension s√©mantique profonde)
- Ne comprend pas les synonymes automatiquement
- Moins performant que les mod√®les neuronaux

---

## 2Ô∏è‚É£ Sentence Transformers (optionnel, via Python)

**Fichier** : `src/Mo5.RagServer.Infrastructure/Services/LocalEmbeddingService.cs`

### C'est quoi ?

Un **mod√®le neuronal** (r√©seau de neurones) qui comprend le sens du texte.

**Pas de NuGet**, mais utilise **Python** + biblioth√®que `sentence-transformers`.

### Comment √ßa marche ?

#### √âtape 1 : V√©rification de Python

Le service v√©rifie que Python est install√© sur le syst√®me.

```csharp
// V√©rifier que Python est disponible
if (!IsPythonAvailable())
{
    throw new InvalidOperationException("Python is not available. Please install Python 3.8+ and ensure it's in PATH.");
}
```

Si Python n'est pas trouv√©, le service retourne une erreur.

#### √âtape 2 : Installation des d√©pendances Python

Si n√©cessaire, le service installe automatiquement les packages Python requis :
- `sentence-transformers` - Biblioth√®que de mod√®les d'embeddings
- `torch` - Framework de deep learning (PyTorch)
- `numpy` - Calculs num√©riques

```csharp
private void EnsurePythonDependencies()
{
    var requiredPackages = new[] { "sentence-transformers", "torch", "numpy" };

    foreach (var package in requiredPackages)
    {
        if (!IsPythonPackageInstalled(package))
        {
            _logger.LogInformation("Installing Python package: {Package}", package);
            InstallPythonPackage(package);
        }
    }
}
```

**Note** : L'installation se fait automatiquement au premier d√©marrage (peut prendre quelques minutes).

#### √âtape 3 : G√©n√©ration de l'embedding via Python

Le service C# g√©n√®re un **script Python** et l'ex√©cute.

**Script Python g√©n√©r√©** :

```python
import json
import sys
from sentence_transformers import SentenceTransformer

try:
    # Load model (cached after first use)
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    # Input texts
    texts = ["Le registre A est un accumulateur"]

    # Generate embeddings
    embeddings = model.encode(texts, convert_to_numpy=True)

    # Convert to list and output as JSON
    result = embeddings.tolist()
    print(json.dumps(result))

except Exception as e:
    print(f'Error: {e}', file=sys.stderr)
    sys.exit(1)
```

**Mod√®le utilis√©** : `paraphrase-multilingual-MiniLM-L12-v2`
- Multilingue (fran√ßais, anglais, etc.)
- Optimis√© pour la paraphrase (comprend les synonymes)
- 384 dimensions
- Taille : ~420 MB (t√©l√©charg√© au premier usage)

#### √âtape 4 : R√©cup√©ration du r√©sultat

Le service C# r√©cup√®re le JSON retourn√© par Python et le d√©s√©rialise.

```csharp
private async Task<float[]> GenerateEmbeddingInternalAsync(string text, CancellationToken cancellationToken)
{
    var pythonScript = CreatePythonScript(new[] { text });
    var result = await ExecutePythonScriptAsync(pythonScript, cancellationToken);

    var embeddings = JsonSerializer.Deserialize<float[][]>(result);
    return embeddings?[0] ?? throw new InvalidOperationException("Failed to parse embedding result");
}
```

**R√©sultat** : Un vecteur de **384 nombres** (float) g√©n√©r√© par le mod√®le neuronal.

### Comment fonctionne le mod√®le neuronal ?

**C'est un r√©seau de neurones** entra√Æn√© sur des millions de phrases.

**Principe** :
1. Le texte est d√©coup√© en tokens (mots ou sous-mots)
2. Chaque token passe dans plusieurs couches de neurones
3. Le mod√®le "comprend" le contexte et le sens
4. Il produit un vecteur de 384 dimensions

**Exemple** :

```
Texte : "Le registre A est un accumulateur"

Tokens : ["Le", "registre", "A", "est", "un", "accumulateur"]
    ‚Üì
R√©seau de neurones (12 couches)
    ‚Üì
Vecteur : [0.23, -0.45, 0.12, ..., 0.67]  (384 dimensions)
```

**Magie** : Le mod√®le comprend que "registre accumulateur" et "accumulator register" ont le m√™me sens, m√™me si les mots sont diff√©rents !

### Avantages et inconv√©nients

**‚úÖ Avantages** :
- Compr√©hension s√©mantique profonde
- Comprend les synonymes automatiquement
- Multilingue (fran√ßais, anglais, etc.)
- Qualit√© sup√©rieure pour la recherche

**‚ùå Inconv√©nients** :
- N√©cessite Python + d√©pendances
- Plus lent (surtout au premier d√©marrage)
- T√©l√©charge un mod√®le de ~420 MB
- Plus complexe √† configurer

---

## üìä Comparaison des deux m√©thodes

| Crit√®re | TF-IDF Simple | Sentence Transformers |
|---------|---------------|----------------------|
| **Technologie** | Formule math√©matique | R√©seau de neurones |
| **Langage** | 100% C# | C# + Python |
| **D√©pendances** | Aucune | Python + packages |
| **Qualit√©** | Bonne (mots-cl√©s) | Excellente (s√©mantique) |
| **Vitesse** | Tr√®s rapide | Plus lent (1√®re fois) |
| **Compr√©hension** | Bas√©e sur les mots | Comprend le sens |
| **Multilingue** | Oui (avec filtres) | Oui (mod√®le multilingue) |
| **Taille** | ~50 KB de code | ~420 MB de mod√®le |
| **Configuration** | Aucune | Python 3.8+ requis |

---

## üéØ Quelle m√©thode utiliser ?

### Utilisez TF-IDF Simple si :

- Vous voulez une solution **simple et rapide**
- Vous n'avez pas Python install√©
- Vous d√©veloppez sur un environnement contraint (Raspberry Pi, etc.)
- Votre documentation utilise des termes techniques pr√©cis

### Utilisez Sentence Transformers si :

- Vous voulez la **meilleure qualit√©** de recherche
- Vous avez Python install√© (ou pouvez l'installer)
- Vous avez de l'espace disque (~500 MB)
- Votre documentation contient des synonymes, paraphrases, etc.

---

## üîß Configuration

### TF-IDF Simple (par d√©faut)

**Aucune configuration n√©cessaire !**

Le service est enregistr√© par d√©faut dans `Program.cs` :

```csharp
builder.Services.AddSingleton<IEmbeddingService, SimpleTfIdfEmbeddingService>();
```

### Sentence Transformers (optionnel)

**1. Installer Python 3.8+**

```bash
# V√©rifier la version
python --version
```

**2. Modifier `Program.cs`**

Remplacer :

```csharp
builder.Services.AddSingleton<IEmbeddingService, SimpleTfIdfEmbeddingService>();
```

Par :

```csharp
builder.Services.AddSingleton<IEmbeddingService, LocalEmbeddingService>();
```

**3. (Optionnel) Configurer le mod√®le dans `appsettings.json`**

```json
{
  "LocalEmbedding": {
    "ModelName": "paraphrase-multilingual-MiniLM-L12-v2",
    "PythonPath": "python"
  }
}
```

**4. D√©marrer le serveur**

Au premier d√©marrage, les d√©pendances Python seront install√©es automatiquement.

---

## üß™ Exemple concret

### Avec TF-IDF Simple

**Texte** : "Le registre A est un accumulateur 8 bits"

**Processus** :
1. Filtrage : ["registre", "accumulateur", "bits"]
2. Calcul TF : registre=0.33, accumulateur=0.33, bits=0.33
3. Calcul IDF : registre=0.22, accumulateur=3.00, bits=1.50
4. TF-IDF : [0.07, 1.00, 0.50, 0, 0, ..., 0]
5. Normalisation : [0.05, 0.71, 0.35, 0, 0, ..., 0]

**R√©sultat** : Vecteur de 384 dimensions

### Avec Sentence Transformers

**Texte** : "Le registre A est un accumulateur 8 bits"

**Processus** :
1. Tokenisation : ["Le", "registre", "A", "est", "un", "accumulateur", "8", "bits"]
2. Passage dans le r√©seau de neurones (12 couches)
3. Agr√©gation des repr√©sentations
4. Normalisation

**R√©sultat** : Vecteur de 384 dimensions (valeurs diff√©rentes de TF-IDF)

---

## üí¨ R√©sum√©

### TF-IDF Simple

**C'est une formule math√©matique** :
1. On compte la fr√©quence des mots (TF)
2. On calcule la raret√© des mots (IDF)
3. On multiplie les deux (TF √ó IDF)
4. On normalise le vecteur

**Pas de NuGet externe**, juste du code C# avec des calculs math√©matiques.

### Sentence Transformers

**C'est un mod√®le neuronal** :
1. Le texte est pass√© dans un r√©seau de neurones
2. Le mod√®le a √©t√© entra√Æn√© sur des millions de phrases
3. Il comprend le **sens** du texte, pas juste les mots
4. Il retourne un vecteur de 384 dimensions

**Utilise Python** + biblioth√®que `sentence-transformers` (qui elle-m√™me utilise PyTorch).

---

**Les deux m√©thodes produisent des vecteurs de 384 nombres qui peuvent √™tre compar√©s avec la similarit√© cosinus.** üöÄ


