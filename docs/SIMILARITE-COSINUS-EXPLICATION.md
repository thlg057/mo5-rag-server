# ğŸ“ SimilaritÃ© Cosinus - Comment on compare les embeddings ?

> Une explication simple de la similaritÃ© cosinus et comment elle permet de trouver les documents pertinents

## Le problÃ¨me

On a gÃ©nÃ©rÃ© des embeddings (vecteurs de 384 nombres) pour tous les chunks de documentation.

On a aussi gÃ©nÃ©rÃ© un embedding pour la question de l'utilisateur.

**Maintenant, comment savoir quels chunks sont les plus pertinents ?**

Il faut **comparer** l'embedding de la question avec tous les embeddings des chunks.

Mais comment comparer deux listes de 384 nombres ? ğŸ¤”

---

## La solution : SimilaritÃ© cosinus

La **similaritÃ© cosinus** est une formule mathÃ©matique qui mesure Ã  quel point deux vecteurs sont "proches".

### L'idÃ©e de base

Imaginez deux vecteurs comme des **flÃ¨ches** dans l'espace.

```
        â†— Vecteur A (question)
       /
      /
     /____â†’ Vecteur B (chunk)
```

**SimilaritÃ© cosinus** = mesure de l'**angle** entre les deux flÃ¨ches.

- Si les flÃ¨ches pointent dans la **mÃªme direction** â†’ angle petit â†’ similaritÃ© Ã©levÃ©e
- Si les flÃ¨ches pointent dans des **directions opposÃ©es** â†’ angle grand â†’ similaritÃ© faible

### Le score

La similaritÃ© cosinus donne un **score entre -1 et 1** :

- **1** = Vecteurs identiques (mÃªme direction)
- **0** = Vecteurs perpendiculaires (aucun lien)
- **-1** = Vecteurs opposÃ©s (sens contraire)

Dans notre cas, les vecteurs sont normalisÃ©s (longueur = 1), donc le score est gÃ©nÃ©ralement entre **0 et 1**.

**InterprÃ©tation** :

- **0.9 - 1.0** : TrÃ¨s similaire â­â­â­
- **0.7 - 0.9** : Similaire â­â­
- **0.5 - 0.7** : Moyennement similaire â­
- **0.0 - 0.5** : Peu similaire

---

## La formule mathÃ©matique

### Formule complÃ¨te

```
similaritÃ©_cosinus = (A Â· B) / (||A|| Ã— ||B||)
```

**OÃ¹** :
- **A Â· B** = produit scalaire (dot product)
- **||A||** = magnitude (longueur) du vecteur A
- **||B||** = magnitude (longueur) du vecteur B

### Ã‰tape par Ã©tape

#### 1. Produit scalaire (dot product)

On multiplie chaque dimension des deux vecteurs et on additionne :

```
A = [0.5, 0.3, 0.1]
B = [0.6, 0.2, 0.4]

A Â· B = (0.5 Ã— 0.6) + (0.3 Ã— 0.2) + (0.1 Ã— 0.4)
      = 0.30 + 0.06 + 0.04
      = 0.40
```

#### 2. Magnitude (longueur) des vecteurs

On calcule la "longueur" de chaque vecteur avec le thÃ©orÃ¨me de Pythagore :

```
||A|| = âˆš(0.5Â² + 0.3Â² + 0.1Â²)
      = âˆš(0.25 + 0.09 + 0.01)
      = âˆš0.35
      = 0.59

||B|| = âˆš(0.6Â² + 0.2Â² + 0.4Â²)
      = âˆš(0.36 + 0.04 + 0.16)
      = âˆš0.56
      = 0.75
```

#### 3. Division

On divise le produit scalaire par le produit des magnitudes :

```
similaritÃ©_cosinus = 0.40 / (0.59 Ã— 0.75)
                   = 0.40 / 0.44
                   = 0.91
```

**RÃ©sultat** : Score de **0.91** â†’ TrÃ¨s similaire ! â­â­â­

---

## ImplÃ©mentation en C#

Voici le code utilisÃ© dans le projet :

```csharp
private static float CalculateCosineSimilarity(float[] vectorA, float[] vectorB)
{
    if (vectorA.Length != vectorB.Length)
        return 0f;

    double dotProduct = 0;
    double magnitudeA = 0;
    double magnitudeB = 0;

    // Calculer le produit scalaire et les magnitudes en une seule passe
    for (int i = 0; i < vectorA.Length; i++)
    {
        dotProduct += vectorA[i] * vectorB[i];      // A Â· B
        magnitudeA += vectorA[i] * vectorA[i];      // ||A||Â²
        magnitudeB += vectorB[i] * vectorB[i];      // ||B||Â²
    }

    // Calculer les racines carrÃ©es des magnitudes
    magnitudeA = Math.Sqrt(magnitudeA);
    magnitudeB = Math.Sqrt(magnitudeB);

    // Ã‰viter la division par zÃ©ro
    if (magnitudeA == 0 || magnitudeB == 0)
        return 0f;

    // Calculer la similaritÃ© cosinus
    return (float)(dotProduct / (magnitudeA * magnitudeB));
}
```


#### 2. Calcul des similaritÃ©s

Le systÃ¨me calcule la similaritÃ© cosinus entre la question et chaque chunk :

```
SimilaritÃ©(Q, C1) = CalculateCosineSimilarity([0.5, 0.3, 0.8, ...], [0.52, 0.28, 0.75, ...])
                  = 0.94  â­â­â­ (trÃ¨s similaire)

SimilaritÃ©(Q, C2) = CalculateCosineSimilarity([0.5, 0.3, 0.8, ...], [-0.1, 0.6, -0.3, ...])
                  = 0.23  (peu similaire)

SimilaritÃ©(Q, C3) = CalculateCosineSimilarity([0.5, 0.3, 0.8, ...], [0.48, 0.32, 0.77, ...])
                  = 0.92  â­â­â­ (trÃ¨s similaire)
```

#### 3. Tri par score

Les chunks sont triÃ©s par score dÃ©croissant :

```
1. Chunk 1 : 0.94 â­â­â­
2. Chunk 3 : 0.92 â­â­â­
3. Chunk 2 : 0.23
```

#### 4. Filtrage par seuil

On ne garde que les chunks avec un score supÃ©rieur au seuil (par exemple 0.5) :

```
RÃ©sultats finaux :
1. Chunk 1 : 0.94 â­â­â­
2. Chunk 3 : 0.92 â­â­â­
```

**RÃ©sultat** : L'utilisateur reÃ§oit les 2 chunks les plus pertinents sur le registre X !

---

## Pourquoi la similaritÃ© cosinus ?

### Avantages

**âœ… IndÃ©pendante de la longueur**

La similaritÃ© cosinus mesure l'**angle**, pas la longueur.

Deux vecteurs peuvent avoir des longueurs diffÃ©rentes mais pointer dans la mÃªme direction â†’ score Ã©levÃ©.

**Exemple** :

```
A = [1, 2, 3]
B = [2, 4, 6]  (2 fois plus long que A)

SimilaritÃ© cosinus = 1.0  (mÃªme direction)
```

**âœ… NormalisÃ©e entre -1 et 1**

Le score est toujours dans la mÃªme plage, facile Ã  interprÃ©ter.

**âœ… Rapide Ã  calculer**

Une seule boucle sur les 384 dimensions, trÃ¨s efficace.

**âœ… Fonctionne bien avec les embeddings**

Les embeddings reprÃ©sentent le "sens" du texte, et la similaritÃ© cosinus mesure la "proximitÃ© sÃ©mantique".

### Alternatives

Il existe d'autres mÃ©thodes pour comparer des vecteurs :

**Distance euclidienne** :
- Mesure la distance "en ligne droite" entre deux points
- Sensible Ã  la longueur des vecteurs
- Moins adaptÃ©e aux embeddings

**Distance de Manhattan** :
- Somme des diffÃ©rences absolues sur chaque dimension
- Moins utilisÃ©e pour les embeddings

**Produit scalaire (dot product)** :
- Plus simple mais sensible Ã  la longueur
- NÃ©cessite des vecteurs normalisÃ©s

**SimilaritÃ© cosinus** est le meilleur choix pour les embeddings ! ğŸ¯

---

## Optimisation avec pgvector

### Le problÃ¨me de performance

Si on a 10 000 chunks dans la base, il faut calculer 10 000 similaritÃ©s pour chaque recherche.

Avec 384 dimensions par vecteur, Ã§a fait beaucoup de calculs ! ğŸ˜…

### La solution : Index IVFFlat

**pgvector** utilise un index spÃ©cial appelÃ© **IVFFlat** (Inverted File with Flat compression).

**Principe** :
1. Les vecteurs sont regroupÃ©s en **clusters** (groupes)
2. Lors d'une recherche, on cherche d'abord les clusters les plus proches
3. Puis on cherche les vecteurs les plus proches **dans ces clusters**

**RÃ©sultat** : Au lieu de comparer avec 10 000 vecteurs, on compare avec ~100 vecteurs.

**Gain de performance** : 10x Ã  100x plus rapide ! ğŸš€

### Configuration de l'index

Dans le projet, l'index est crÃ©Ã© avec cette commande SQL :

```sql
CREATE INDEX "IX_DocumentChunks_Embedding"
ON "DocumentChunks"
USING ivfflat ("Embedding" vector_cosine_ops)
WITH (lists = 100);
```

**ParamÃ¨tres** :
- **ivfflat** : Type d'index (Inverted File with Flat compression)
- **vector_cosine_ops** : OpÃ©rateur de similaritÃ© cosinus
- **lists = 100** : Nombre de clusters (100 groupes)

**Recommandation** : `lists = nombre_de_lignes / 1000` (pour 10 000 chunks â†’ 10 clusters)

---

## Utilisation dans le code

### Recherche avec similaritÃ©

Voici comment le systÃ¨me utilise la similaritÃ© cosinus pour la recherche :

```csharp
public async Task<SearchResponse> SearchAsync(SearchRequest request, CancellationToken cancellationToken)
{
    // 1. GÃ©nÃ©rer l'embedding de la question
    var queryEmbedding = await _embeddingService.GenerateEmbeddingAsync(request.Query, cancellationToken);

    // 2. RÃ©cupÃ©rer les chunks de la base
    var chunks = await _context.DocumentChunks
        .Include(dc => dc.Document)
        .Where(dc => dc.Document.IsActive)
        .Take(1000)
        .ToListAsync(cancellationToken);

    // 3. Calculer la similaritÃ© pour chaque chunk
    var results = chunks
        .Select(chunk => new
        {
            Chunk = chunk,
            SimilarityScore = CalculateCosineSimilarity(chunk.Embedding.ToArray(), queryEmbedding.ToArray())
        })
        // 4. Filtrer par score minimum
        .Where(x => x.SimilarityScore >= request.MinSimilarityScore)
        // 5. Trier par score dÃ©croissant
        .OrderByDescending(x => x.SimilarityScore)
        // 6. Prendre les N meilleurs rÃ©sultats
        .Take(request.MaxResults)
        .ToList();

    return new SearchResponse
    {
        Query = request.Query,
        Results = results,
        TotalResults = results.Count
    };
}
```

### ParamÃ¨tres configurables

**MinSimilarityScore** : Score minimum pour qu'un chunk soit retournÃ©
- Par dÃ©faut : **0.5**
- Valeurs typiques : 0.3 Ã  0.8
- Plus le seuil est Ã©levÃ©, plus les rÃ©sultats sont prÃ©cis (mais moins nombreux)

**MaxResults** : Nombre maximum de rÃ©sultats Ã  retourner
- Par dÃ©faut : **5**
- Valeurs typiques : 3 Ã  20
- Plus il y a de rÃ©sultats, plus il y a de contexte (mais plus de bruit)

---

## Exemple avec des valeurs rÃ©elles

### Vecteurs simplifiÃ©s (3 dimensions au lieu de 384)

Pour mieux comprendre, voici un exemple avec des vecteurs de 3 dimensions :

**Question** : "registre accumulateur"
```
Embedding Q = [0.8, 0.5, 0.2]
```

**Chunk 1** : "Le registre A est un accumulateur 8 bits"
```
Embedding C1 = [0.75, 0.48, 0.25]
```

**Chunk 2** : "La mÃ©moire vidÃ©o est situÃ©e Ã  0x4000"
```
Embedding C2 = [0.1, 0.9, -0.3]
```

### Calcul pour Chunk 1

```
1. Produit scalaire :
   A Â· B = (0.8 Ã— 0.75) + (0.5 Ã— 0.48) + (0.2 Ã— 0.25)
         = 0.60 + 0.24 + 0.05
         = 0.89

2. Magnitude de Q :
   ||Q|| = âˆš(0.8Â² + 0.5Â² + 0.2Â²)
         = âˆš(0.64 + 0.25 + 0.04)
         = âˆš0.93
         = 0.96

3. Magnitude de C1 :
   ||C1|| = âˆš(0.75Â² + 0.48Â² + 0.25Â²)
          = âˆš(0.56 + 0.23 + 0.06)
          = âˆš0.85
          = 0.92

4. SimilaritÃ© cosinus :
   cos(Q, C1) = 0.89 / (0.96 Ã— 0.92)
              = 0.89 / 0.88
              = 1.01 â‰ˆ 1.0  â­â­â­
```

**RÃ©sultat** : Score de **1.0** â†’ Quasi identique !

### Calcul pour Chunk 2

```
1. Produit scalaire :
   A Â· B = (0.8 Ã— 0.1) + (0.5 Ã— 0.9) + (0.2 Ã— -0.3)
         = 0.08 + 0.45 - 0.06
         = 0.47

2. Magnitude de Q : 0.96 (dÃ©jÃ  calculÃ©e)

3. Magnitude de C2 :
   ||C2|| = âˆš(0.1Â² + 0.9Â² + (-0.3)Â²)
          = âˆš(0.01 + 0.81 + 0.09)
          = âˆš0.91
          = 0.95

4. SimilaritÃ© cosinus :
   cos(Q, C2) = 0.47 / (0.96 Ã— 0.95)
              = 0.47 / 0.91
              = 0.52
```

**RÃ©sultat** : Score de **0.52** â†’ Moyennement similaire

**Conclusion** : Chunk 1 (1.0) est beaucoup plus pertinent que Chunk 2 (0.52) !

---

## Visualisation gÃ©omÃ©trique

### ReprÃ©sentation 2D (simplifiÃ©e)

Imaginons des vecteurs en 2D pour visualiser :

```
      Y
      â†‘
      |
      |    â†— Q (question)
      |   /
      |  / 15Â°
      | /___â†’ C1 (chunk 1)
      |
      |
      |        â†— C2 (chunk 2)
      |       /
      |      / 60Â°
      |     /
      |____/________________â†’ X
```

**Angle petit (15Â°)** â†’ SimilaritÃ© Ã©levÃ©e (cos(15Â°) â‰ˆ 0.97)
**Angle grand (60Â°)** â†’ SimilaritÃ© moyenne (cos(60Â°) = 0.50)

Plus l'angle est petit, plus les vecteurs pointent dans la mÃªme direction, plus ils sont similaires.

---

## ğŸ’¬ RÃ©sumÃ©

### Qu'est-ce que la similaritÃ© cosinus ?

Une **formule mathÃ©matique** qui mesure l'angle entre deux vecteurs.

**Score de -1 Ã  1** :
- **1** = Identique
- **0** = Aucun lien
- **-1** = OpposÃ©

### Comment Ã§a marche ?

**3 Ã©tapes** :
1. Calculer le **produit scalaire** (A Â· B)
2. Calculer les **magnitudes** (||A|| et ||B||)
3. Diviser : **similaritÃ© = (A Â· B) / (||A|| Ã— ||B||)**

### Pourquoi c'est utile ?

**âœ… Compare les embeddings** : Trouve les chunks les plus proches de la question
**âœ… Rapide** : Une seule boucle sur les 384 dimensions
**âœ… NormalisÃ©** : Score toujours entre -1 et 1
**âœ… OptimisÃ©** : Index IVFFlat de pgvector pour la performance

### Dans le RAG Server

1. Question â†’ Embedding Q
2. Pour chaque chunk â†’ Calculer similaritÃ©(Q, chunk)
3. Trier par score dÃ©croissant
4. Retourner les top N rÃ©sultats

**C'est grÃ¢ce Ã  la similaritÃ© cosinus qu'on peut trouver les passages pertinents dans la documentation !** ğŸš€

**Optimisation** : On calcule tout en une seule boucle pour Ãªtre plus rapide.

---

## Exemple concret avec le RAG Server

### ScÃ©nario

Vous posez la question : **"Comment utiliser le registre X ?"**

Le systÃ¨me a 3 chunks dans la base :

**Chunk 1** : "Le registre X est utilisÃ© pour l'indexation mÃ©moire..."
**Chunk 2** : "La mÃ©moire vidÃ©o du MO5 est situÃ©e Ã  l'adresse..."
**Chunk 3** : "Le registre X permet d'accÃ©der aux tableaux..."

### Processus

#### 1. GÃ©nÃ©ration des embeddings

```
Question : "Comment utiliser le registre X ?"
â†’ Embedding Q : [0.5, 0.3, 0.8, ..., 0.2]  (384 dimensions)

Chunk 1 : "Le registre X est utilisÃ© pour l'indexation..."
â†’ Embedding C1 : [0.52, 0.28, 0.75, ..., 0.19]

Chunk 2 : "La mÃ©moire vidÃ©o du MO5..."
â†’ Embedding C2 : [-0.1, 0.6, -0.3, ..., 0.5]

Chunk 3 : "Le registre X permet d'accÃ©der aux tableaux..."
â†’ Embedding C3 : [0.48, 0.32, 0.77, ..., 0.21]
```


